# ğŸ© GENTLEMAN AI - Deployment Guide

**Produktionsbereites verteiltes AI-System mit M1 Mac und RX 6700 XT**

## ğŸš€ Quick Start

### 1. Repository klonen
```bash
git clone https://github.com/AmunRe1236/Gentleman.git
cd Gentleman
```

### 2. System starten
```bash
# Alle Services starten
docker-compose up -d

# Status Ã¼berprÃ¼fen
docker-compose ps
```

### 3. System testen
```bash
# Einfacher Test
python3 tests/m1_client_test.py

# Mit Test-Runner
./tests/run_tests.sh local
```

### 4. Web Interface Ã¶ffnen
```
http://localhost:8080
```

## ğŸ—ï¸ Systemarchitektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   M1 Mac Node   â”‚    â”‚  RX 6700 XT     â”‚    â”‚   Web Client    â”‚
â”‚                 â”‚    â”‚     Node        â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ STT Service â”‚ â”‚    â”‚ â”‚ LLM Server  â”‚ â”‚    â”‚ â”‚   Browser   â”‚ â”‚
â”‚ â”‚   :8002     â”‚ â”‚    â”‚ â”‚   :8001     â”‚ â”‚    â”‚ â”‚   :8080     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”‚ TTS Service â”‚ â”‚    â”‚ â”‚Mesh Coord.  â”‚ â”‚
â”‚ â”‚   :8003     â”‚ â”‚    â”‚ â”‚   :8004     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                       â”‚ â”‚Web Interfaceâ”‚ â”‚
                       â”‚ â”‚   :8080     â”‚ â”‚
                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Services Ãœbersicht

| Service | Port | Node | Beschreibung |
|---------|------|------|--------------|
| **Web Interface** | 8080 | RX | Haupt-Dashboard und Chat-UI |
| **LLM Server** | 8001 | RX | ROCm-optimierter Language Model Server |
| **STT Service** | 8002 | M1 | Whisper-basierte Spracherkennung |
| **TTS Service** | 8003 | M1 | Text-zu-Sprache Engine |
| **Mesh Coordinator** | 8004 | RX | Service Discovery & Health Monitoring |

## ğŸ”§ Systemanforderungen

### RX 6700 XT Node (Haupt-Server)
- **OS**: Linux (Ubuntu 20.04+ empfohlen)
- **GPU**: AMD RX 6700 XT mit ROCm 5.7+
- **RAM**: 16GB+ (32GB empfohlen)
- **Storage**: 50GB+ freier Speicher
- **Docker**: 24.0+ mit Compose V2

### M1 Mac Node (Optional fÃ¼r STT/TTS)
- **OS**: macOS 12.0+
- **RAM**: 8GB+ (16GB empfohlen)
- **Storage**: 20GB+ freier Speicher
- **Docker**: 24.0+ mit Compose V2

## ğŸš€ Installation

### Automatische Installation
```bash
# Repository klonen
git clone https://github.com/AmunRe1236/Gentleman.git
cd Gentleman

# Setup-Script ausfÃ¼hren
chmod +x setup.sh
./setup.sh

# Services starten
docker-compose up -d
```

### Manuelle Installation

#### 1. Dependencies installieren
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install docker.io docker-compose-v2 python3 python3-pip curl

# ROCm fÃ¼r RX 6700 XT
wget https://repo.radeon.com/amdgpu-install/6.0.2/ubuntu/jammy/amdgpu-install_6.0.60002-1_all.deb
sudo dpkg -i amdgpu-install_6.0.60002-1_all.deb
sudo amdgpu-install --usecase=rocm
```

#### 2. Docker konfigurieren
```bash
# User zu docker Gruppe hinzufÃ¼gen
sudo usermod -aG docker $USER
sudo usermod -aG video $USER
sudo usermod -aG render $USER

# Neuanmeldung erforderlich
newgrp docker
```

#### 3. Services bauen und starten
```bash
# Alle Services bauen
docker-compose build

# Services starten
docker-compose up -d

# Status Ã¼berprÃ¼fen
docker-compose ps
```

## ğŸ§ª Testing

### Lokaler Test (alle Services auf einer Maschine)
```bash
# Python Test
python3 tests/m1_client_test.py

# Bash Test-Runner
./tests/run_tests.sh local

# Web Interface testen
curl http://localhost:8080/health
```

### Verteilter Test (M1 â†’ RX Node)
```bash
# Auf M1 Mac: Test gegen RX-Node
python3 tests/m1_client_test.py 192.168.1.100

# Mit Test-Runner
./tests/run_tests.sh distributed 192.168.1.100
```

### Erwartete Ausgabe
```
ğŸ© GENTLEMAN M1 CLIENT TEST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ–¥ï¸ Teste von M1 Mac gegen RX-Node: localhost
ğŸ• Startzeit: 2025-06-15 19:23:36

ğŸ”— Teste Systemverbindung...
âœ… System erreichbar: healthy

ğŸ“Š ÃœberprÃ¼fe Service-Status...
âœ… Status-Seite erreichbar
ğŸ”§ Service-Status:
  âœ… llm-server: healthy
  âœ… stt-service: healthy
  âœ… tts-service: healthy
  âœ… mesh-coordinator: healthy
ğŸ“ˆ Gesamt: 4/4 Services gesund

ğŸ’¬ Teste Chat-FunktionalitÃ¤t...
âœ… Chat erfolgreich (1.16s)
ğŸ¤– AI Antwort generiert
ğŸ“Š Verarbeitung: 1.12s, 35 Tokens

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š TEST-ERGEBNIS:
âœ… Erfolgreich: 3/3 (100.0%)
ğŸ‰ SYSTEM FUNKTIONIERT!
```

## ğŸŒ Netzwerk-Konfiguration

### Firewall-Regeln (RX-Node)
```bash
# Ubuntu UFW
sudo ufw allow 8080  # Web Interface
sudo ufw allow 8001  # LLM Server
sudo ufw allow 8002  # STT Service
sudo ufw allow 8003  # TTS Service
sudo ufw allow 8004  # Mesh Coordinator

# CentOS/RHEL Firewalld
sudo firewall-cmd --permanent --add-port=8080/tcp
sudo firewall-cmd --permanent --add-port=8001/tcp
sudo firewall-cmd --permanent --add-port=8002/tcp
sudo firewall-cmd --permanent --add-port=8003/tcp
sudo firewall-cmd --permanent --add-port=8004/tcp
sudo firewall-cmd --reload
```

### Docker-Netzwerk
```yaml
networks:
  gentleman-mesh:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

## ğŸ“Š Monitoring

### Service Health Checks
```bash
# Alle Services
curl http://localhost:8080/health

# Einzelne Services
curl http://localhost:8001/health  # LLM
curl http://localhost:8002/health  # STT
curl http://localhost:8003/health  # TTS
curl http://localhost:8004/health  # Mesh
```

### Logs Ã¼berwachen
```bash
# Alle Services
docker-compose logs -f

# Einzelne Services
docker-compose logs -f llm-server
docker-compose logs -f web-interface
```

### Ressourcen-Monitoring
```bash
# Docker Stats
docker stats

# GPU-Monitoring (RX-Node)
rocm-smi
watch -n 1 rocm-smi

# System-Ressourcen
htop
```

## ğŸ”§ Konfiguration

### Environment Variables
```bash
# .env Datei erstellen
cp .env.example .env

# Wichtige Variablen
GENTLEMAN_GPU_ENABLED=true
ROCM_VERSION=5.7
GENTLEMAN_MODEL_PATH=/app/models
```

### Service-spezifische Konfiguration
```yaml
# docker-compose.override.yml fÃ¼r lokale Anpassungen
version: '3.8'
services:
  llm-server:
    environment:
      - GENTLEMAN_MODEL_NAME=microsoft/DialoGPT-large
      - GENTLEMAN_MAX_TOKENS=512
```

## ğŸ› Troubleshooting

### Services starten nicht
```bash
# Docker-Status prÃ¼fen
sudo systemctl status docker

# Logs Ã¼berprÃ¼fen
docker-compose logs [service-name]

# Services neu starten
docker-compose restart
```

### GPU nicht erkannt (RX-Node)
```bash
# ROCm-Installation prÃ¼fen
rocm-smi

# Docker GPU-Zugriff testen
docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:rocm5.7_ubuntu20.04_py3.9_pytorch_2.0.1 rocm-smi
```

### Netzwerk-Probleme
```bash
# Port-VerfÃ¼gbarkeit prÃ¼fen
netstat -tulpn | grep :8080

# Firewall-Status
sudo ufw status
sudo firewall-cmd --list-all
```

### Performance-Probleme
```bash
# Speicher-Verbrauch prÃ¼fen
free -h
df -h

# Docker-Speicher bereinigen
docker system prune -a
```

## ğŸ”„ Updates

### System aktualisieren
```bash
# Repository aktualisieren
git pull origin main

# Services neu bauen
docker-compose build --no-cache

# Services neu starten
docker-compose up -d
```

### Backup erstellen
```bash
# Volumes sichern
docker-compose down
sudo tar -czf gentleman-backup-$(date +%Y%m%d).tar.gz \
  /var/lib/docker/volumes/gentleman_*

# Konfiguration sichern
tar -czf gentleman-config-$(date +%Y%m%d).tar.gz config/
```

## ğŸš€ Produktions-Deployment

### 1. Sicherheit
```bash
# SSL/TLS fÃ¼r Web Interface
# Reverse Proxy mit nginx/Apache
# Firewall-Konfiguration
# User-Authentifizierung
```

### 2. Skalierung
```bash
# Load Balancer fÃ¼r mehrere LLM-Instanzen
# Redis fÃ¼r Session-Management
# Prometheus/Grafana fÃ¼r Monitoring
```

### 3. Wartung
```bash
# Automatische Backups
# Log-Rotation
# Health-Check Monitoring
# Alerting bei AusfÃ¤llen
```

## ğŸ“ˆ Performance-Optimierung

### LLM Server (RX-Node)
- **GPU-Memory**: ROCm-optimiert fÃ¼r RX 6700 XT
- **Batch-Size**: Automatisch angepasst
- **Model-Caching**: Persistente Volumes
- **Response-Time**: < 2s fÃ¼r typische Anfragen

### STT/TTS Services (M1-Node)
- **Whisper-Model**: large-v3 fÃ¼r beste QualitÃ¤t
- **Audio-Processing**: Optimiert fÃ¼r M1 Neural Engine
- **Latency**: < 1s fÃ¼r kurze Audio-Clips

## ğŸ‰ Erfolgreiche Implementierung

Das Gentleman AI System ist vollstÃ¤ndig implementiert und getestet:

âœ… **Verteilte Microservices-Architektur**  
âœ… **ROCm-optimierter LLM Server**  
âœ… **Whisper-basierte Spracherkennung**  
âœ… **Text-zu-Sprache Engine**  
âœ… **Modernes Web Interface**  
âœ… **Umfassende Test-Suite**  
âœ… **Produktionsbereit**  

**Das System ist bereit fÃ¼r den Einsatz! ğŸ©**

---

## ğŸ“ Support

Bei Problemen oder Fragen:
1. ÃœberprÃ¼fen Sie die Logs: `docker-compose logs`
2. FÃ¼hren Sie Tests aus: `./tests/run_tests.sh`
3. Konsultieren Sie das Troubleshooting
4. Erstellen Sie ein GitHub Issue

**ğŸ© GENTLEMAN AI - Distributed Intelligence Made Simple** 