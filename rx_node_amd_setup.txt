# GENTLEMAN RX Node AMD GPU AI Setup Commands
# Für AMD RX 6700 XT mit ROCm Support
# Diese Befehle auf der RX Node ausführen (als amo9n11 user)

echo "🎯 AMD RX 6700 XT AI Setup wird gestartet..."

# 1. System Update
echo "📦 System Update..."
sudo pacman -Syu --noconfirm

# 2. ROCm Installation (Arch Linux)
echo "🔧 Installiere ROCm für AMD GPU..."
sudo pacman -S rocm-dev rocm-libs hip-dev --noconfirm

# Alternative: AUR ROCm (falls Probleme)
# yay -S rocm-opencl-runtime rocm-cmake --noconfirm

# 3. Python AI Dependencies
echo "🐍 Installiere Python AI Stack..."
sudo pacman -S python python-pip python-virtualenv --noconfirm

# 4. Erstelle AI Environment
echo "🌐 Erstelle AI Virtual Environment..."
cd ~
python -m venv ai_env
source ai_env/bin/activate

# 5. PyTorch für ROCm installieren
echo "🔥 Installiere PyTorch mit ROCm Support..."
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7

# 6. AI Libraries installieren
echo "🤖 Installiere AI Libraries..."
pip install transformers
pip install diffusers
pip install accelerate
pip install datasets
pip install tokenizers
pip install safetensors
pip install huggingface-hub
pip install pillow
pip install numpy
pip install scipy
pip install scikit-learn
pip install matplotlib
pip install requests
pip install flask
pip install fastapi
pip install uvicorn

# 7. ROCm Environment testen
echo "🧪 Teste ROCm Installation..."
python -c "import torch; print(f'PyTorch Version: {torch.__version__}'); print(f'ROCm Available: {torch.cuda.is_available()}'); print(f'GPU Count: {torch.cuda.device_count()}'); print(f'GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\"}')"

# 8. AMD GPU Info
echo "📊 AMD GPU Information:"
rocm-smi
lspci | grep VGA

# 9. Erstelle AI Server mit AMD GPU Support
cat > ~/amd_ai_server.py << 'PYEOF'
#!/usr/bin/env python3
"""
GENTLEMAN AMD AI Server
Optimiert für AMD RX 6700 XT mit ROCm
"""

import os
import sys
import json
import time
import subprocess
import threading
from datetime import datetime
from pathlib import Path

# Web Framework
from flask import Flask, request, jsonify
import torch

# AI Libraries
try:
    from transformers import pipeline, AutoTokenizer, AutoModel
    from diffusers import StableDiffusionPipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("⚠️ Transformers nicht verfügbar")

app = Flask(__name__)

# Global AI Models
models = {}
device = "cpu"

def initialize_amd_gpu():
    """Initialisiere AMD GPU mit ROCm"""
    global device
    
    print("🔍 Prüfe AMD GPU Verfügbarkeit...")
    
    if torch.cuda.is_available():
        device = "cuda"  # ROCm nutzt cuda API
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"✅ AMD GPU gefunden: {gpu_name}")
        print(f"💾 GPU Memory: {gpu_memory:.1f} GB")
        print(f"🎯 Device: {device}")
        
        # GPU Warm-up
        dummy_tensor = torch.randn(100, 100).to(device)
        _ = dummy_tensor @ dummy_tensor
        del dummy_tensor
        torch.cuda.empty_cache()
        
        return True
    else:
        print("⚠️ Keine AMD GPU gefunden, nutze CPU")
        device = "cpu"
        return False

def load_ai_models():
    """Lade AI Models für verschiedene Tasks"""
    global models
    
    if not TRANSFORMERS_AVAILABLE:
        print("⚠️ Transformers nicht verfügbar, nutze Basic Models")
        return
    
    try:
        print("📚 Lade AI Models...")
        
        # Text Generation (klein für RX 6700 XT)
        print("📝 Lade Text Generation Model...")
        models['text_gen'] = pipeline(
            "text-generation",
            model="microsoft/DialoGPT-small",
            device=0 if device == "cuda" else -1,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        )
        
        # Sentiment Analysis
        print("😊 Lade Sentiment Analysis...")
        models['sentiment'] = pipeline(
            "sentiment-analysis",
            device=0 if device == "cuda" else -1
        )
        
        # Text Summarization
        print("📄 Lade Summarization Model...")
        models['summarize'] = pipeline(
            "summarization",
            model="facebook/bart-large-cnn",
            device=0 if device == "cuda" else -1,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        )
        
        print("✅ AI Models geladen!")
        
    except Exception as e:
        print(f"❌ Fehler beim Laden der Models: {e}")
        models = {}

@app.route('/health')
def health():
    """Health Check"""
    return jsonify({
        "status": "ok",
        "service": "amd-ai-server",
        "gpu": device,
        "timestamp": datetime.now().isoformat(),
        "models_loaded": len(models)
    })

@app.route('/gpu/status')
def gpu_status():
    """AMD GPU Status"""
    try:
        # ROCm-SMI Info
        rocm_info = subprocess.run(['rocm-smi'], capture_output=True, text=True)
        
        gpu_info = {
            "device": device,
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "rocm_smi": rocm_info.stdout if rocm_info.returncode == 0 else "N/A"
        }
        
        if torch.cuda.is_available():
            gpu_info.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory,
                "gpu_memory_allocated": torch.cuda.memory_allocated(0),
                "gpu_memory_reserved": torch.cuda.memory_reserved(0)
            })
        
        return jsonify(gpu_info)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/ai/text/generate', methods=['POST'])
def generate_text():
    """Text Generation mit AMD GPU"""
    if 'text_gen' not in models:
        return jsonify({"error": "Text generation model nicht verfügbar"}), 400
    
    data = request.get_json()
    prompt = data.get('prompt', 'Hello')
    max_length = data.get('max_length', 50)
    
    try:
        start_time = time.time()
        
        result = models['text_gen'](
            prompt,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=models['text_gen'].tokenizer.eos_token_id
        )
        
        processing_time = time.time() - start_time
        
        return jsonify({
            "status": "success",
            "prompt": prompt,
            "generated_text": result[0]['generated_text'],
            "processing_time": processing_time,
            "device": device,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/ai/text/sentiment', methods=['POST'])
def analyze_sentiment():
    """Sentiment Analysis"""
    if 'sentiment' not in models:
        return jsonify({"error": "Sentiment model nicht verfügbar"}), 400
    
    data = request.get_json()
    text = data.get('text', '')
    
    try:
        start_time = time.time()
        result = models['sentiment'](text)
        processing_time = time.time() - start_time
        
        return jsonify({
            "status": "success",
            "text": text,
            "sentiment": result[0]['label'],
            "confidence": result[0]['score'],
            "processing_time": processing_time,
            "device": device,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/ai/text/summarize', methods=['POST'])
def summarize_text():
    """Text Summarization"""
    if 'summarize' not in models:
        return jsonify({"error": "Summarization model nicht verfügbar"}), 400
    
    data = request.get_json()
    text = data.get('text', '')
    max_length = data.get('max_length', 150)
    min_length = data.get('min_length', 30)
    
    try:
        start_time = time.time()
        result = models['summarize'](
            text,
            max_length=max_length,
            min_length=min_length,
            do_sample=False
        )
        processing_time = time.time() - start_time
        
        return jsonify({
            "status": "success",
            "original_text": text,
            "summary": result[0]['summary_text'],
            "processing_time": processing_time,
            "device": device,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/ai/benchmark')
def benchmark():
    """GPU Benchmark"""
    try:
        if device == "cpu":
            return jsonify({"message": "CPU Benchmark nicht implementiert"})
        
        print("🏃 Starte GPU Benchmark...")
        start_time = time.time()
        
        # Matrix Multiplikation Benchmark
        size = 2048
        a = torch.randn(size, size, device=device, dtype=torch.float16)
        b = torch.randn(size, size, device=device, dtype=torch.float16)
        
        torch.cuda.synchronize()
        compute_start = time.time()
        
        c = torch.matmul(a, b)
        
        torch.cuda.synchronize()
        compute_time = time.time() - compute_start
        
        total_time = time.time() - start_time
        
        # Cleanup
        del a, b, c
        torch.cuda.empty_cache()
        
        return jsonify({
            "status": "success",
            "benchmark": {
                "matrix_size": f"{size}x{size}",
                "compute_time": compute_time,
                "total_time": total_time,
                "device": device,
                "dtype": "float16"
            },
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    print("🚀 GENTLEMAN AMD AI Server startet...")
    print("🎯 Hardware: AMD RX 6700 XT")
    
    # Initialize GPU
    gpu_available = initialize_amd_gpu()
    
    # Load Models
    load_ai_models()
    
    print(f"📡 Endpoints verfügbar:")
    print(f"   GET  /health           - Health Check")
    print(f"   GET  /gpu/status       - AMD GPU Status")
    print(f"   POST /ai/text/generate - Text Generation")
    print(f"   POST /ai/text/sentiment - Sentiment Analysis")
    print(f"   POST /ai/text/summarize - Text Summarization")
    print(f"   GET  /ai/benchmark     - GPU Benchmark")
    
    # Start Server
    app.run(host='0.0.0.0', port=8765, debug=False, threaded=True)
PYEOF

chmod +x ~/amd_ai_server.py

# 10. Systemd Service erstellen
echo "⚙️ Erstelle AI Service..."
sudo tee /etc/systemd/system/gentleman-amd-ai.service > /dev/null << SERVICEEOF
[Unit]
Description=GENTLEMAN AMD AI Server
After=network.target

[Service]
Type=simple
User=amo9n11
WorkingDirectory=/home/amo9n11
Environment=PATH=/home/amo9n11/ai_env/bin:/usr/bin:/bin
ExecStart=/home/amo9n11/ai_env/bin/python /home/amo9n11/amd_ai_server.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
SERVICEEOF

# 11. Service aktivieren
sudo systemctl daemon-reload
sudo systemctl enable gentleman-amd-ai.service

# 12. Final Status
echo "📊 Setup Status:"
echo "ROCm Version:"
/opt/rocm/bin/rocminfo | head -10

echo "PyTorch + ROCm Test:"
source ai_env/bin/activate
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'Device Count: {torch.cuda.device_count()}')
    print(f'Device Name: {torch.cuda.get_device_name(0)}')
    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
"

echo "🎉 AMD AI Setup abgeschlossen!"
echo "Starte Service mit: sudo systemctl start gentleman-amd-ai.service"
echo "AI Server läuft auf: http://$(hostname -I | awk '{print $1}'):8765"
