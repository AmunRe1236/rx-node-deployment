# GENTLEMAN RX Node AMD GPU AI Setup Commands
# FÃ¼r AMD RX 6700 XT mit ROCm Support
# Diese Befehle auf der RX Node ausfÃ¼hren (als amo9n11 user)

echo "ðŸŽ¯ AMD RX 6700 XT AI Setup wird gestartet..."

# 1. System Update
echo "ðŸ“¦ System Update..."
sudo pacman -Syu --noconfirm

# 2. ROCm Installation (Arch Linux)
echo "ðŸ”§ Installiere ROCm fÃ¼r AMD GPU..."
sudo pacman -S rocm-dev rocm-libs hip-dev --noconfirm

# Alternative: AUR ROCm (falls Probleme)
# yay -S rocm-opencl-runtime rocm-cmake --noconfirm

# 3. Python AI Dependencies
echo "ðŸ Installiere Python AI Stack..."
sudo pacman -S python python-pip python-virtualenv --noconfirm

# 4. Erstelle AI Environment
echo "ðŸŒ Erstelle AI Virtual Environment..."
cd ~
python -m venv ai_env
source ai_env/bin/activate

# 5. PyTorch fÃ¼r ROCm installieren
echo "ðŸ”¥ Installiere PyTorch mit ROCm Support..."
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7

# 6. AI Libraries installieren
echo "ðŸ¤– Installiere AI Libraries..."
pip install transformers
pip install diffusers
pip install accelerate
pip install datasets
pip install tokenizers
pip install safetensors
pip install huggingface-hub
pip install pillow
pip install numpy
pip install scipy
pip install scikit-learn
pip install matplotlib
pip install requests
pip install flask
pip install fastapi
pip install uvicorn

# 7. ROCm Environment testen
echo "ðŸ§ª Teste ROCm Installation..."
python -c "import torch; print(f'PyTorch Version: {torch.__version__}'); print(f'ROCm Available: {torch.cuda.is_available()}'); print(f'GPU Count: {torch.cuda.device_count()}'); print(f'GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\"}')"

# 8. AMD GPU Info
echo "ðŸ“Š AMD GPU Information:"
rocm-smi
lspci | grep VGA

# 9. Erstelle AI Server mit AMD GPU Support
cat > ~/amd_ai_server.py << 'PYEOF'
#!/usr/bin/env python3
"""
GENTLEMAN AMD AI Server
Optimiert fÃ¼r AMD RX 6700 XT mit ROCm
"""

import os
import sys
import json
import time
import subprocess
import threading
from datetime import datetime
from pathlib import Path

# Web Framework
from flask import Flask, request, jsonify
import torch

# AI Libraries
try:
    from transformers import pipeline, AutoTokenizer, AutoModel
    from diffusers import StableDiffusionPipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Transformers nicht verfÃ¼gbar")

app = Flask(__name__)

# Global AI Models
models = {}
device = "cpu"

def initialize_amd_gpu():
    """Initialisiere AMD GPU mit ROCm"""
    global device
    
    print("ðŸ” PrÃ¼fe AMD GPU VerfÃ¼gbarkeit...")
    
    if torch.cuda.is_available():
        device = "cuda"  # ROCm nutzt cuda API
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"âœ… AMD GPU gefunden: {gpu_name}")
        print(f"ðŸ’¾ GPU Memory: {gpu_memory:.1f} GB")
        print(f"ðŸŽ¯ Device: {device}")
        
        # GPU Warm-up
        dummy_tensor = torch.randn(100, 100).to(device)
        _ = dummy_tensor @ dummy_tensor
        del dummy_tensor
        torch.cuda.empty_cache()
        
        return True
    else:
        print("âš ï¸ Keine AMD GPU gefunden, nutze CPU")
        device = "cpu"
        return False

def load_ai_models():
    """Lade AI Models fÃ¼r verschiedene Tasks"""
    global models
    
    if not TRANSFORMERS_AVAILABLE:
        print("âš ï¸ Transformers nicht verfÃ¼gbar, nutze Basic Models")
        return
    
    try:
        print("ðŸ“š Lade AI Models...")
        
        # Text Generation (klein fÃ¼r RX 6700 XT)
        print("ðŸ“ Lade Text Generation Model...")
        models['text_gen'] = pipeline(
            "text-generation",
            model="microsoft/DialoGPT-small",
            device=0 if device == "cuda" else -1,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        )
        
        # Sentiment Analysis
        print("ðŸ˜Š Lade Sentiment Analysis...")
        models['sentiment'] = pipeline(
            "sentiment-analysis",
            device=0 if device == "cuda" else -1
        )
        
        # Text Summarization
        print("ðŸ“„ Lade Summarization Model...")
        models['summarize'] = pipeline(
            "summarization",
            model="facebook/bart-large-cnn",
            device=0 if device == "cuda" else -1,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        )
        
        print("âœ… AI Models geladen!")
        
    except Exception as e:
        print(f"âŒ Fehler beim Laden der Models: {e}")
        models = {}

@app.route('/health')
def health():
    """Health Check"""
    return jsonify({
        "status": "ok",
        "service": "amd-ai-server",
        "gpu": device,
        "timestamp": datetime.now().isoformat(),
        "models_loaded": len(models)
    })

@app.route('/gpu/status')
def gpu_status():
    """AMD GPU Status"""
    try:
        # ROCm-SMI Info
        rocm_info = subprocess.run(['rocm-smi'], capture_output=True, text=True)
        
        gpu_info = {
            "device": device,
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "rocm_smi": rocm_info.stdout if rocm_info.returncode == 0 else "N/A"
        }
        
        if torch.cuda.is_available():
            gpu_info.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory,
                "gpu_memory_allocated": torch.cuda.memory_allocated(0),
                "gpu_memory_reserved": torch.cuda.memory_reserved(0)
            })
        
        return jsonify(gpu_info)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/ai/text/generate', methods=['POST'])
def generate_text():
    """Text Generation mit AMD GPU"""
    if 'text_gen' not in models:
        return jsonify({"error": "Text generation model nicht verfÃ¼gbar"}), 400
    
    data = request.get_json()
    prompt = data.get('prompt', 'Hello')
    max_length = data.get('max_length', 50)
    
    try:
        start_time = time.time()
        
        result = models['text_gen'](
            prompt,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=models['text_gen'].tokenizer.eos_token_id
        )
        
        processing_time = time.time() - start_time
        
        return jsonify({
            "status": "success",
            "prompt": prompt,
            "generated_text": result[0]['generated_text'],
            "processing_time": processing_time,
            "device": device,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/ai/text/sentiment', methods=['POST'])
def analyze_sentiment():
    """Sentiment Analysis"""
    if 'sentiment' not in models:
        return jsonify({"error": "Sentiment model nicht verfÃ¼gbar"}), 400
    
    data = request.get_json()
    text = data.get('text', '')
    
    try:
        start_time = time.time()
        result = models['sentiment'](text)
        processing_time = time.time() - start_time
        
        return jsonify({
            "status": "success",
            "text": text,
            "sentiment": result[0]['label'],
            "confidence": result[0]['score'],
            "processing_time": processing_time,
            "device": device,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/ai/text/summarize', methods=['POST'])
def summarize_text():
    """Text Summarization"""
    if 'summarize' not in models:
        return jsonify({"error": "Summarization model nicht verfÃ¼gbar"}), 400
    
    data = request.get_json()
    text = data.get('text', '')
    max_length = data.get('max_length', 150)
    min_length = data.get('min_length', 30)
    
    try:
        start_time = time.time()
        result = models['summarize'](
            text,
            max_length=max_length,
            min_length=min_length,
            do_sample=False
        )
        processing_time = time.time() - start_time
        
        return jsonify({
            "status": "success",
            "original_text": text,
            "summary": result[0]['summary_text'],
            "processing_time": processing_time,
            "device": device,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/ai/benchmark')
def benchmark():
    """GPU Benchmark"""
    try:
        if device == "cpu":
            return jsonify({"message": "CPU Benchmark nicht implementiert"})
        
        print("ðŸƒ Starte GPU Benchmark...")
        start_time = time.time()
        
        # Matrix Multiplikation Benchmark
        size = 2048
        a = torch.randn(size, size, device=device, dtype=torch.float16)
        b = torch.randn(size, size, device=device, dtype=torch.float16)
        
        torch.cuda.synchronize()
        compute_start = time.time()
        
        c = torch.matmul(a, b)
        
        torch.cuda.synchronize()
        compute_time = time.time() - compute_start
        
        total_time = time.time() - start_time
        
        # Cleanup
        del a, b, c
        torch.cuda.empty_cache()
        
        return jsonify({
            "status": "success",
            "benchmark": {
                "matrix_size": f"{size}x{size}",
                "compute_time": compute_time,
                "total_time": total_time,
                "device": device,
                "dtype": "float16"
            },
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    print("ðŸš€ GENTLEMAN AMD AI Server startet...")
    print("ðŸŽ¯ Hardware: AMD RX 6700 XT")
    
    # Initialize GPU
    gpu_available = initialize_amd_gpu()
    
    # Load Models
    load_ai_models()
    
    print(f"ðŸ“¡ Endpoints verfÃ¼gbar:")
    print(f"   GET  /health           - Health Check")
    print(f"   GET  /gpu/status       - AMD GPU Status")
    print(f"   POST /ai/text/generate - Text Generation")
    print(f"   POST /ai/text/sentiment - Sentiment Analysis")
    print(f"   POST /ai/text/summarize - Text Summarization")
    print(f"   GET  /ai/benchmark     - GPU Benchmark")
    
    # Start Server
    app.run(host='0.0.0.0', port=8765, debug=False, threaded=True)
PYEOF

chmod +x ~/amd_ai_server.py

# 10. Systemd Service erstellen
echo "âš™ï¸ Erstelle AI Service..."
sudo tee /etc/systemd/system/gentleman-amd-ai.service > /dev/null << SERVICEEOF
[Unit]
Description=GENTLEMAN AMD AI Server
After=network.target

[Service]
Type=simple
User=amo9n11
WorkingDirectory=/home/amo9n11
Environment=PATH=/home/amo9n11/ai_env/bin:/usr/bin:/bin
ExecStart=/home/amo9n11/ai_env/bin/python /home/amo9n11/amd_ai_server.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
SERVICEEOF

# 11. Service aktivieren
sudo systemctl daemon-reload
sudo systemctl enable gentleman-amd-ai.service

# 12. Final Status
echo "ðŸ“Š Setup Status:"
echo "ROCm Version:"
/opt/rocm/bin/rocminfo | head -10

echo "PyTorch + ROCm Test:"
source ai_env/bin/activate
python -c "
import torch
print(f'PyTorch: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'Device Count: {torch.cuda.device_count()}')
    print(f'Device Name: {torch.cuda.get_device_name(0)}')
    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
"

echo "ðŸŽ‰ AMD AI Setup abgeschlossen!"
echo "Starte Service mit: sudo systemctl start gentleman-amd-ai.service"
echo "AI Server lÃ¤uft auf: http://$(hostname -I | awk '{print $1}'):8765"
