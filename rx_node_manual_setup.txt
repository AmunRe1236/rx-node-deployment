# GENTLEMAN RX Node Tailscale + AI Setup Commands
# Diese Befehle auf der RX Node ausführen (als amo9n11 user)

# 1. Tailscale Installation (Arch Linux)
echo "🔧 Installiere Tailscale..."
sudo pacman -S tailscale --noconfirm

# 2. Tailscale Service starten
echo "🚀 Starte Tailscale Service..."
sudo systemctl enable tailscaled
sudo systemctl start tailscaled

# 3. Tailscale Netzwerk beitreten
echo "🔗 Verbinde mit Tailscale..."
sudo tailscale up --advertise-routes=192.168.68.0/24 --accept-routes

# 4. Tailscale Status prüfen
echo "📊 Tailscale Status:"
tailscale status
tailscale ip -4

# 5. AI Services Setup
echo "🤖 Erstelle AI Services..."

# Python AI Server erstellen
cat > ~/ai_server.py << 'PYEOF'
#!/usr/bin/env python3
import http.server
import socketserver
import json
import subprocess
import os
import threading
import time
from datetime import datetime

class AIHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            response = {
                "status": "ok", 
                "service": "rx-node-ai", 
                "timestamp": datetime.now().isoformat(),
                "capabilities": ["text-processing", "image-analysis", "data-computation"]
            }
            self.wfile.write(json.dumps(response).encode())
            
        elif self.path == '/ai/status':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            
            # System Info für AI
            gpu_info = "N/A"
            try:
                gpu_info = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], 
                                        capture_output=True, text=True).stdout.strip()
            except:
                pass
            
            cpu_info = subprocess.run(['lscpu'], capture_output=True, text=True).stdout
            memory_info = subprocess.run(['free', '-h'], capture_output=True, text=True).stdout
            
            response = {
                "status": "ready",
                "service": "rx-node-ai",
                "hardware": {
                    "gpu": gpu_info,
                    "cpu_cores": os.cpu_count(),
                    "memory": memory_info.split('\n')[1].split()[1] if memory_info else "Unknown"
                },
                "models": ["text-generation", "image-processing", "data-analysis"],
                "timestamp": datetime.now().isoformat()
            }
            self.wfile.write(json.dumps(response).encode())
            
        elif self.path.startswith('/ai/'):
            # AI Endpoints
            self.handle_ai_request()
        else:
            self.send_error(404)
    
    def do_POST(self):
        if self.path.startswith('/ai/'):
            self.handle_ai_request()
        else:
            self.send_error(404)
    
    def handle_ai_request(self):
        """Handle AI processing requests"""
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length) if content_length > 0 else b''
        
        try:
            request_data = json.loads(post_data.decode()) if post_data else {}
        except:
            request_data = {}
        
        if self.path == '/ai/text/process':
            self.process_text(request_data)
        elif self.path == '/ai/image/analyze':
            self.analyze_image(request_data)
        elif self.path == '/ai/compute':
            self.compute_data(request_data)
        else:
            self.send_error(404)
    
    def process_text(self, data):
        """Simulate text processing"""
        text = data.get('text', 'No text provided')
        
        # Simulate processing time
        time.sleep(0.1)
        
        result = {
            "status": "processed",
            "input_length": len(text),
            "word_count": len(text.split()),
            "processed_text": text.upper(),  # Simple transformation
            "processing_time": 0.1,
            "timestamp": datetime.now().isoformat()
        }
        
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(result).encode())
    
    def analyze_image(self, data):
        """Simulate image analysis"""
        image_path = data.get('image_path', 'no_image')
        
        result = {
            "status": "analyzed",
            "image": image_path,
            "analysis": {
                "objects_detected": ["example_object1", "example_object2"],
                "confidence": 0.95,
                "processing_time": 0.5
            },
            "timestamp": datetime.now().isoformat()
        }
        
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(result).encode())
    
    def compute_data(self, data):
        """Simulate data computation"""
        numbers = data.get('numbers', [1, 2, 3, 4, 5])
        operation = data.get('operation', 'sum')
        
        if operation == 'sum':
            result_value = sum(numbers)
        elif operation == 'average':
            result_value = sum(numbers) / len(numbers) if numbers else 0
        elif operation == 'max':
            result_value = max(numbers) if numbers else 0
        else:
            result_value = 0
        
        result = {
            "status": "computed",
            "operation": operation,
            "input": numbers,
            "result": result_value,
            "timestamp": datetime.now().isoformat()
        }
        
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(result).encode())

if __name__ == "__main__":
    PORT = 8765
    with socketserver.TCPServer(("", PORT), AIHandler) as httpd:
        print(f"🤖 RX Node AI Server läuft auf Port {PORT}")
        print(f"🎯 Verfügbare AI Endpoints:")
        print(f"   GET  /health          - Health Check")
        print(f"   GET  /ai/status       - AI System Status")
        print(f"   POST /ai/text/process - Text Processing")
        print(f"   POST /ai/image/analyze - Image Analysis")
        print(f"   POST /ai/compute      - Data Computation")
        httpd.serve_forever()
PYEOF

chmod +x ~/ai_server.py

# 6. AI Server als Service einrichten
echo "⚙️ Erstelle AI Service..."
sudo tee /etc/systemd/system/gentleman-ai.service > /dev/null << SERVICEEOF
[Unit]
Description=GENTLEMAN AI Server
After=network.target tailscaled.service

[Service]
Type=simple
User=amo9n11
WorkingDirectory=/home/amo9n11
ExecStart=/usr/bin/python3 /home/amo9n11/ai_server.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
SERVICEEOF

# Service aktivieren
sudo systemctl daemon-reload
sudo systemctl enable gentleman-ai.service
sudo systemctl start gentleman-ai.service

# 7. Status prüfen
echo "📊 Final Status Check:"
echo "Tailscale Status:"
tailscale status
echo ""
echo "Tailscale IP:"
tailscale ip -4
echo ""
echo "AI Service Status:"
systemctl status gentleman-ai.service --no-pager -l
echo ""
echo "🎉 Setup abgeschlossen!"
echo "AI Server läuft auf: http://$(tailscale ip -4):8765"
