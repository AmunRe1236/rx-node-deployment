# ğŸš€ GENTLEMAN RX Node Deployment

Automatisches Deployment-System fÃ¼r RX Node mit AMD GPU-Beschleunigung

## ğŸ¯ Schnelle Installation

```bash
# 1. Repository klonen
git clone http://192.168.68.111:8080/rx-node-deployment.git
cd rx-node-deployment

# 2. Deployment starten
chmod +x quick_rx_deployment.sh
./quick_rx_deployment.sh
```

## ğŸ® Was wird installiert:

- âœ… **AMD ROCm GPU-Treiber** fÃ¼r GPU-Beschleunigung
- âœ… **LM Studio v0.2.29** mit GPU-Support  
- âœ… **Firewall-Konfiguration** (Port 1234)
- âœ… **Systemd Auto-Start Service**
- âœ… **GPU-Test-Skripte**

## ğŸ”Š Nach dem Deployment:

1. **LM Studio starten**: `./start_lm_studio.sh`
2. **Modell downloaden** (empfohlen: deepseek-r1-7b)
3. **Server aktivieren** auf Port 1234 mit GPU
4. **â¡ï¸ GPU-LÃ¼fter werden hÃ¶rbar sein! ğŸ®**

## ğŸ“‹ Deployment-Optionen:

### Option 1: VollstÃ¤ndiges Deployment
```bash
./quick_rx_deployment.sh
```

### Option 2: Kompaktes Deployment  
```bash
./rx_deployment_direct.sh
```

### Option 3: Direkter Befehl
```bash
# Siehe rx_direct_deploy_command.txt
```

## ğŸŒ Server URLs nach Installation:

- **LM Studio API**: `http://192.168.68.117:1234`
- **Models Endpoint**: `http://192.168.68.117:1234/v1/models`
- **Chat Completions**: `http://192.168.68.117:1234/v1/chat/completions`

## ğŸ§ª GPU-Test:

Nach der Installation:
```bash
python3 test_gpu_inference.py
```

Erwartete Ergebnisse:
- âš¡ Antwortzeit: <30 Sekunden
- ğŸš€ Token-Rate: >15 tokens/sec
- ğŸ”Š **GPU-LÃ¼fter hÃ¶rbar wÃ¤hrend Inferenz**

---

**GENTLEMAN Dynamic Cluster System**  
RX Node (192.168.68.117) - AI Inference Engine 