# GENTLEMAN AMD GPU AI Setup Guide

## ðŸŽ¯ Hardware Ãœbersicht

**RX Node Spezifikationen:**
- **GPU**: AMD Radeon RX 6700 XT (Navi 22, 12GB VRAM)
- **CPU**: AMD Ryzen 5 1600 (6 Cores)
- **RAM**: 16GB DDR4
- **OS**: Arch Linux
- **User**: amo9n11

---

## ðŸ”§ Voraussetzungen

### 1. Tailscale Setup abgeschlossen âœ…
- RX Node in Tailscale Netzwerk integriert
- SSH Zugriff Ã¼ber Tailscale IP funktioniert
- Siehe: `GENTLEMAN_Tailscale_Setup_Guide.md`

### 2. SSH Verbindung zur RX Node
```bash
# Via lokales Netzwerk
ssh rx-node  # oder ssh amo9n11@192.168.68.117

# Via Tailscale (nach Setup)
ssh amo9n11@100.x.x.x  # RX Node Tailscale IP
```

---

## ðŸš€ AMD GPU Setup Schritte

### Schritt 1: System Update
```bash
# Arch Linux System Update
sudo pacman -Syu

# Kernel Headers installieren (falls nÃ¶tig)
sudo pacman -S linux-headers
```

### Schritt 2: ROCm Installation
```bash
# ROCm Pakete installieren
sudo pacman -S rocm-dev rocm-libs hip-dev

# Alternative falls Probleme auftreten:
# yay -S rocm-opencl-runtime rocm-cmake

# ROCm Version prÃ¼fen
/opt/rocm/bin/rocminfo | head -10
```

### Schritt 3: GPU Status verifizieren
```bash
# AMD GPU Info
lspci | grep VGA
# Sollte zeigen: AMD/ATI Navi 22 [Radeon RX 6700/6700 XT]

# ROCm GPU Detection
rocm-smi

# Kernel Module prÃ¼fen
lsmod | grep amdgpu
```

### Schritt 4: Python AI Environment
```bash
# Python Dependencies
sudo pacman -S python python-pip python-virtualenv

# AI Virtual Environment erstellen
cd ~
python -m venv ai_env
source ai_env/bin/activate

# Environment aktivieren fÃ¼r zukÃ¼nftige Sessions
echo "source ~/ai_env/bin/activate" >> ~/.bashrc
```

### Schritt 5: PyTorch mit ROCm
```bash
# Stelle sicher, dass ai_env aktiv ist
source ai_env/bin/activate

# Pip Update
pip install --upgrade pip

# PyTorch fÃ¼r ROCm 5.7 (kompatibel mit RX 6700 XT)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7

# PyTorch Installation testen
python -c "
import torch
print(f'PyTorch Version: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'Device Count: {torch.cuda.device_count()}')
    print(f'Device Name: {torch.cuda.get_device_name(0)}')
"
```

### Schritt 6: AI Libraries Installation
```bash
# Basis AI Libraries
pip install transformers
pip install diffusers
pip install accelerate
pip install datasets
pip install tokenizers
pip install safetensors
pip install huggingface-hub

# ZusÃ¤tzliche Dependencies
pip install pillow numpy scipy scikit-learn matplotlib
pip install requests flask fastapi uvicorn

# Jupyter fÃ¼r Experimente (optional)
pip install jupyter notebook
```

---

## ðŸ¤– AI Server Setup

### AMD AI Server erstellen
```bash
# AI Server Script erstellen
cat > ~/amd_ai_server.py << 'EOF'
#!/usr/bin/env python3
"""
GENTLEMAN AMD AI Server
Optimiert fÃ¼r AMD RX 6700 XT mit ROCm
"""

import os
import sys
import json
import time
import subprocess
import threading
from datetime import datetime
from pathlib import Path

# Web Framework
from flask import Flask, request, jsonify
import torch

# AI Libraries (mit Error Handling)
try:
    from transformers import pipeline, AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    print("âœ… Transformers verfÃ¼gbar")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Transformers nicht verfÃ¼gbar")

app = Flask(__name__)

# Global AI Models
models = {}
device = "cpu"

def initialize_amd_gpu():
    """Initialisiere AMD GPU mit ROCm"""
    global device
    
    print("ðŸ” PrÃ¼fe AMD GPU VerfÃ¼gbarkeit...")
    
    if torch.cuda.is_available():
        device = "cuda"  # ROCm nutzt cuda API
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"âœ… AMD GPU gefunden: {gpu_name}")
        print(f"ðŸ’¾ GPU Memory: {gpu_memory:.1f} GB")
        print(f"ðŸŽ¯ Device: {device}")
        
        # GPU Warm-up
        dummy_tensor = torch.randn(100, 100).to(device)
        _ = dummy_tensor @ dummy_tensor
        del dummy_tensor
        torch.cuda.empty_cache()
        
        return True
    else:
        print("âš ï¸ Keine AMD GPU gefunden, nutze CPU")
        device = "cpu"
        return False

@app.route('/health')
def health():
    """Health Check"""
    return jsonify({
        "status": "ok",
        "service": "amd-ai-server",
        "gpu": device,
        "timestamp": datetime.now().isoformat(),
        "models_loaded": len(models)
    })

@app.route('/gpu/status')
def gpu_status():
    """AMD GPU Status"""
    try:
        # ROCm-SMI Info
        rocm_info = subprocess.run(['rocm-smi'], capture_output=True, text=True)
        
        gpu_info = {
            "device": device,
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "rocm_smi": rocm_info.stdout if rocm_info.returncode == 0 else "N/A"
        }
        
        if torch.cuda.is_available():
            gpu_info.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory,
                "gpu_memory_allocated": torch.cuda.memory_allocated(0),
                "gpu_memory_reserved": torch.cuda.memory_reserved(0)
            })
        
        return jsonify(gpu_info)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    print("ðŸš€ GENTLEMAN AMD AI Server startet...")
    print("ðŸŽ¯ Hardware: AMD RX 6700 XT")
    
    # Initialize GPU
    gpu_available = initialize_amd_gpu()
    
    print(f"ðŸ“¡ Endpoints verfÃ¼gbar:")
    print(f"   GET  /health      - Health Check")
    print(f"   GET  /gpu/status  - AMD GPU Status")
    
    # Start Server
    app.run(host='0.0.0.0', port=8765, debug=False, threaded=True)
EOF

chmod +x ~/amd_ai_server.py
```

### Systemd Service erstellen
```bash
# Service File erstellen
sudo tee /etc/systemd/system/gentleman-amd-ai.service > /dev/null << 'EOF'
[Unit]
Description=GENTLEMAN AMD AI Server
After=network.target

[Service]
Type=simple
User=amo9n11
WorkingDirectory=/home/amo9n11
Environment=PATH=/home/amo9n11/ai_env/bin:/usr/bin:/bin
ExecStart=/home/amo9n11/ai_env/bin/python /home/amo9n11/amd_ai_server.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

# Service aktivieren
sudo systemctl daemon-reload
sudo systemctl enable gentleman-amd-ai.service
```

---

## ðŸ§ª Testing & Verification

### 1. Manual Testing
```bash
# AI Environment aktivieren
source ~/ai_env/bin/activate

# AI Server manuell starten (fÃ¼r Tests)
python ~/amd_ai_server.py

# In anderem Terminal:
curl http://localhost:8765/health
curl http://localhost:8765/gpu/status
```

### 2. Service Testing
```bash
# Service starten
sudo systemctl start gentleman-amd-ai.service

# Service Status prÃ¼fen
systemctl status gentleman-amd-ai.service

# Logs anzeigen
journalctl -u gentleman-amd-ai.service -f
```

### 3. Remote Testing (vom M1 Mac)
```bash
# RX Node Tailscale IP ermitteln
tailscale status | grep archlinux

# Health Check
curl http://100.x.x.x:8765/health

# GPU Status
curl http://100.x.x.x:8765/gpu/status
```

---

## ðŸ”§ Troubleshooting

### Problem: ROCm nicht gefunden
```bash
# ROCm Installation prÃ¼fen
ls /opt/rocm/

# Environment Variables setzen
export PATH=/opt/rocm/bin:$PATH
export LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH

# In ~/.bashrc hinzufÃ¼gen fÃ¼r permanente LÃ¶sung
echo 'export PATH=/opt/rocm/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
```

### Problem: PyTorch erkennt GPU nicht
```bash
# ROCm Version prÃ¼fen
rocminfo | grep "Agent 2"

# PyTorch ROCm KompatibilitÃ¤t testen
python -c "
import torch
print('ROCm verfÃ¼gbar:', torch.cuda.is_available())
print('GerÃ¤te:', torch.cuda.device_count())
if torch.cuda.is_available():
    print('GPU Name:', torch.cuda.get_device_name(0))
"
```

### Problem: Service startet nicht
```bash
# Service Logs detailliert
journalctl -u gentleman-amd-ai.service --no-pager -l

# Manuelle AusfÃ¼hrung fÃ¼r Debugging
sudo -u amo9n11 /home/amo9n11/ai_env/bin/python /home/amo9n11/amd_ai_server.py

# Permissions prÃ¼fen
ls -la /home/amo9n11/amd_ai_server.py
ls -la /home/amo9n11/ai_env/
```

---

## ðŸ“Š Performance Monitoring

### GPU Monitoring Commands
```bash
# ROCm System Management Interface
rocm-smi

# Kontinuierliches Monitoring
watch -n 1 rocm-smi

# GPU Memory Usage
rocm-smi --showmeminfo

# GPU Temperature
rocm-smi --showtemp
```

### System Resources
```bash
# CPU Usage
htop

# Memory Usage
free -h

# Disk Usage
df -h

# Network Status
ss -tulpn | grep 8765
```

---

## ðŸš€ Erweiterte AI Features

### 1. Text Generation Models
```python
# Kleine Models fÃ¼r RX 6700 XT (12GB)
models = [
    "microsoft/DialoGPT-small",      # 117MB
    "gpt2",                          # 548MB
    "facebook/bart-large-cnn",       # 1.63GB
    "google/flan-t5-base",           # 990MB
]
```

### 2. Image Generation
```python
# Stable Diffusion fÃ¼r RX 6700 XT
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to("cuda")
```

### 3. Model Optimization
```python
# Memory Optimization fÃ¼r 12GB VRAM
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# Model Quantization
model = model.half()  # FP16 fÃ¼r mehr Speed
```

---

## âœ… Setup Verification Checklist

### Hardware
- [ ] AMD RX 6700 XT erkannt (`lspci | grep VGA`)
- [ ] ROCm installiert (`rocm-smi` funktioniert)
- [ ] GPU Memory verfÃ¼gbar (12GB)

### Software
- [ ] Python AI Environment aktiv
- [ ] PyTorch mit ROCm installiert
- [ ] PyTorch erkennt GPU (`torch.cuda.is_available()`)
- [ ] AI Libraries installiert

### Services
- [ ] AI Server startet manuell
- [ ] Systemd Service konfiguriert
- [ ] Service startet automatisch
- [ ] Health Check Ã¼ber Tailscale erreichbar

### Network
- [ ] AI Server auf Port 8765 erreichbar
- [ ] Tailscale Verbindung funktioniert
- [ ] Remote Access vom M1 Mac
- [ ] Firewall konfiguriert

**ðŸŽ‰ AMD GPU AI Setup abgeschlossen wenn alle Checkboxen âœ…**

---

## ðŸ“š NÃ¼tzliche Ressourcen

- [ROCm Documentation](https://rocmdocs.amd.com/)
- [PyTorch ROCm Support](https://pytorch.org/get-started/locally/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [AMD GPU Optimization Guide](https://github.com/RadeonOpenCompute/ROCm) 

## ðŸŽ¯ Hardware Ãœbersicht

**RX Node Spezifikationen:**
- **GPU**: AMD Radeon RX 6700 XT (Navi 22, 12GB VRAM)
- **CPU**: AMD Ryzen 5 1600 (6 Cores)
- **RAM**: 16GB DDR4
- **OS**: Arch Linux
- **User**: amo9n11

---

## ðŸ”§ Voraussetzungen

### 1. Tailscale Setup abgeschlossen âœ…
- RX Node in Tailscale Netzwerk integriert
- SSH Zugriff Ã¼ber Tailscale IP funktioniert
- Siehe: `GENTLEMAN_Tailscale_Setup_Guide.md`

### 2. SSH Verbindung zur RX Node
```bash
# Via lokales Netzwerk
ssh rx-node  # oder ssh amo9n11@192.168.68.117

# Via Tailscale (nach Setup)
ssh amo9n11@100.x.x.x  # RX Node Tailscale IP
```

---

## ðŸš€ AMD GPU Setup Schritte

### Schritt 1: System Update
```bash
# Arch Linux System Update
sudo pacman -Syu

# Kernel Headers installieren (falls nÃ¶tig)
sudo pacman -S linux-headers
```

### Schritt 2: ROCm Installation
```bash
# ROCm Pakete installieren
sudo pacman -S rocm-dev rocm-libs hip-dev

# Alternative falls Probleme auftreten:
# yay -S rocm-opencl-runtime rocm-cmake

# ROCm Version prÃ¼fen
/opt/rocm/bin/rocminfo | head -10
```

### Schritt 3: GPU Status verifizieren
```bash
# AMD GPU Info
lspci | grep VGA
# Sollte zeigen: AMD/ATI Navi 22 [Radeon RX 6700/6700 XT]

# ROCm GPU Detection
rocm-smi

# Kernel Module prÃ¼fen
lsmod | grep amdgpu
```

### Schritt 4: Python AI Environment
```bash
# Python Dependencies
sudo pacman -S python python-pip python-virtualenv

# AI Virtual Environment erstellen
cd ~
python -m venv ai_env
source ai_env/bin/activate

# Environment aktivieren fÃ¼r zukÃ¼nftige Sessions
echo "source ~/ai_env/bin/activate" >> ~/.bashrc
```

### Schritt 5: PyTorch mit ROCm
```bash
# Stelle sicher, dass ai_env aktiv ist
source ai_env/bin/activate

# Pip Update
pip install --upgrade pip

# PyTorch fÃ¼r ROCm 5.7 (kompatibel mit RX 6700 XT)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7

# PyTorch Installation testen
python -c "
import torch
print(f'PyTorch Version: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'Device Count: {torch.cuda.device_count()}')
    print(f'Device Name: {torch.cuda.get_device_name(0)}')
"
```

### Schritt 6: AI Libraries Installation
```bash
# Basis AI Libraries
pip install transformers
pip install diffusers
pip install accelerate
pip install datasets
pip install tokenizers
pip install safetensors
pip install huggingface-hub

# ZusÃ¤tzliche Dependencies
pip install pillow numpy scipy scikit-learn matplotlib
pip install requests flask fastapi uvicorn

# Jupyter fÃ¼r Experimente (optional)
pip install jupyter notebook
```

---

## ðŸ¤– AI Server Setup

### AMD AI Server erstellen
```bash
# AI Server Script erstellen
cat > ~/amd_ai_server.py << 'EOF'
#!/usr/bin/env python3
"""
GENTLEMAN AMD AI Server
Optimiert fÃ¼r AMD RX 6700 XT mit ROCm
"""

import os
import sys
import json
import time
import subprocess
import threading
from datetime import datetime
from pathlib import Path

# Web Framework
from flask import Flask, request, jsonify
import torch

# AI Libraries (mit Error Handling)
try:
    from transformers import pipeline, AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    print("âœ… Transformers verfÃ¼gbar")
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ Transformers nicht verfÃ¼gbar")

app = Flask(__name__)

# Global AI Models
models = {}
device = "cpu"

def initialize_amd_gpu():
    """Initialisiere AMD GPU mit ROCm"""
    global device
    
    print("ðŸ” PrÃ¼fe AMD GPU VerfÃ¼gbarkeit...")
    
    if torch.cuda.is_available():
        device = "cuda"  # ROCm nutzt cuda API
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"âœ… AMD GPU gefunden: {gpu_name}")
        print(f"ðŸ’¾ GPU Memory: {gpu_memory:.1f} GB")
        print(f"ðŸŽ¯ Device: {device}")
        
        # GPU Warm-up
        dummy_tensor = torch.randn(100, 100).to(device)
        _ = dummy_tensor @ dummy_tensor
        del dummy_tensor
        torch.cuda.empty_cache()
        
        return True
    else:
        print("âš ï¸ Keine AMD GPU gefunden, nutze CPU")
        device = "cpu"
        return False

@app.route('/health')
def health():
    """Health Check"""
    return jsonify({
        "status": "ok",
        "service": "amd-ai-server",
        "gpu": device,
        "timestamp": datetime.now().isoformat(),
        "models_loaded": len(models)
    })

@app.route('/gpu/status')
def gpu_status():
    """AMD GPU Status"""
    try:
        # ROCm-SMI Info
        rocm_info = subprocess.run(['rocm-smi'], capture_output=True, text=True)
        
        gpu_info = {
            "device": device,
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "rocm_smi": rocm_info.stdout if rocm_info.returncode == 0 else "N/A"
        }
        
        if torch.cuda.is_available():
            gpu_info.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory,
                "gpu_memory_allocated": torch.cuda.memory_allocated(0),
                "gpu_memory_reserved": torch.cuda.memory_reserved(0)
            })
        
        return jsonify(gpu_info)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    print("ðŸš€ GENTLEMAN AMD AI Server startet...")
    print("ðŸŽ¯ Hardware: AMD RX 6700 XT")
    
    # Initialize GPU
    gpu_available = initialize_amd_gpu()
    
    print(f"ðŸ“¡ Endpoints verfÃ¼gbar:")
    print(f"   GET  /health      - Health Check")
    print(f"   GET  /gpu/status  - AMD GPU Status")
    
    # Start Server
    app.run(host='0.0.0.0', port=8765, debug=False, threaded=True)
EOF

chmod +x ~/amd_ai_server.py
```

### Systemd Service erstellen
```bash
# Service File erstellen
sudo tee /etc/systemd/system/gentleman-amd-ai.service > /dev/null << 'EOF'
[Unit]
Description=GENTLEMAN AMD AI Server
After=network.target

[Service]
Type=simple
User=amo9n11
WorkingDirectory=/home/amo9n11
Environment=PATH=/home/amo9n11/ai_env/bin:/usr/bin:/bin
ExecStart=/home/amo9n11/ai_env/bin/python /home/amo9n11/amd_ai_server.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

# Service aktivieren
sudo systemctl daemon-reload
sudo systemctl enable gentleman-amd-ai.service
```

---

## ðŸ§ª Testing & Verification

### 1. Manual Testing
```bash
# AI Environment aktivieren
source ~/ai_env/bin/activate

# AI Server manuell starten (fÃ¼r Tests)
python ~/amd_ai_server.py

# In anderem Terminal:
curl http://localhost:8765/health
curl http://localhost:8765/gpu/status
```

### 2. Service Testing
```bash
# Service starten
sudo systemctl start gentleman-amd-ai.service

# Service Status prÃ¼fen
systemctl status gentleman-amd-ai.service

# Logs anzeigen
journalctl -u gentleman-amd-ai.service -f
```

### 3. Remote Testing (vom M1 Mac)
```bash
# RX Node Tailscale IP ermitteln
tailscale status | grep archlinux

# Health Check
curl http://100.x.x.x:8765/health

# GPU Status
curl http://100.x.x.x:8765/gpu/status
```

---

## ðŸ”§ Troubleshooting

### Problem: ROCm nicht gefunden
```bash
# ROCm Installation prÃ¼fen
ls /opt/rocm/

# Environment Variables setzen
export PATH=/opt/rocm/bin:$PATH
export LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH

# In ~/.bashrc hinzufÃ¼gen fÃ¼r permanente LÃ¶sung
echo 'export PATH=/opt/rocm/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
```

### Problem: PyTorch erkennt GPU nicht
```bash
# ROCm Version prÃ¼fen
rocminfo | grep "Agent 2"

# PyTorch ROCm KompatibilitÃ¤t testen
python -c "
import torch
print('ROCm verfÃ¼gbar:', torch.cuda.is_available())
print('GerÃ¤te:', torch.cuda.device_count())
if torch.cuda.is_available():
    print('GPU Name:', torch.cuda.get_device_name(0))
"
```

### Problem: Service startet nicht
```bash
# Service Logs detailliert
journalctl -u gentleman-amd-ai.service --no-pager -l

# Manuelle AusfÃ¼hrung fÃ¼r Debugging
sudo -u amo9n11 /home/amo9n11/ai_env/bin/python /home/amo9n11/amd_ai_server.py

# Permissions prÃ¼fen
ls -la /home/amo9n11/amd_ai_server.py
ls -la /home/amo9n11/ai_env/
```

---

## ðŸ“Š Performance Monitoring

### GPU Monitoring Commands
```bash
# ROCm System Management Interface
rocm-smi

# Kontinuierliches Monitoring
watch -n 1 rocm-smi

# GPU Memory Usage
rocm-smi --showmeminfo

# GPU Temperature
rocm-smi --showtemp
```

### System Resources
```bash
# CPU Usage
htop

# Memory Usage
free -h

# Disk Usage
df -h

# Network Status
ss -tulpn | grep 8765
```

---

## ðŸš€ Erweiterte AI Features

### 1. Text Generation Models
```python
# Kleine Models fÃ¼r RX 6700 XT (12GB)
models = [
    "microsoft/DialoGPT-small",      # 117MB
    "gpt2",                          # 548MB
    "facebook/bart-large-cnn",       # 1.63GB
    "google/flan-t5-base",           # 990MB
]
```

### 2. Image Generation
```python
# Stable Diffusion fÃ¼r RX 6700 XT
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to("cuda")
```

### 3. Model Optimization
```python
# Memory Optimization fÃ¼r 12GB VRAM
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# Model Quantization
model = model.half()  # FP16 fÃ¼r mehr Speed
```

---

## âœ… Setup Verification Checklist

### Hardware
- [ ] AMD RX 6700 XT erkannt (`lspci | grep VGA`)
- [ ] ROCm installiert (`rocm-smi` funktioniert)
- [ ] GPU Memory verfÃ¼gbar (12GB)

### Software
- [ ] Python AI Environment aktiv
- [ ] PyTorch mit ROCm installiert
- [ ] PyTorch erkennt GPU (`torch.cuda.is_available()`)
- [ ] AI Libraries installiert

### Services
- [ ] AI Server startet manuell
- [ ] Systemd Service konfiguriert
- [ ] Service startet automatisch
- [ ] Health Check Ã¼ber Tailscale erreichbar

### Network
- [ ] AI Server auf Port 8765 erreichbar
- [ ] Tailscale Verbindung funktioniert
- [ ] Remote Access vom M1 Mac
- [ ] Firewall konfiguriert

**ðŸŽ‰ AMD GPU AI Setup abgeschlossen wenn alle Checkboxen âœ…**

---

## ðŸ“š NÃ¼tzliche Ressourcen

- [ROCm Documentation](https://rocmdocs.amd.com/)
- [PyTorch ROCm Support](https://pytorch.org/get-started/locally/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [AMD GPU Optimization Guide](https://github.com/RadeonOpenCompute/ROCm) 
 